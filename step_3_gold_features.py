# step_3_gold_features.py
"""
BÆ°á»›c 3 - GOLD: Feature Engineering & Pipeline Construction.

Má»¤C TIÃŠU:
- Biáº¿n Ä‘á»•i dá»¯ liá»‡u dáº¡ng chá»¯ (Category, Text) thÃ nh dáº¡ng sá»‘ (Vector).
- XÃ¢y dá»±ng Pipeline xá»­ lÃ½ tá»± Ä‘á»™ng.
- LÆ°u Pipeline Model Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng cho Web App.
"""

import os
from typing import List

from pyspark.sql import SparkSession, DataFrame, functions as F
from pyspark.ml import Pipeline
from pyspark.ml.feature import (
    StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler,
    RegexTokenizer, CountVectorizer
)


# ====== Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN ======
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data2")
MODEL_DIR = os.path.join(BASE_DIR, "models")

SILVER_PATH = os.path.join(DATA_DIR, "silver", "stackoverflow_silver_dev_clean.parquet")
FEATURES_PATH = os.path.join(DATA_DIR, "salary_features_spark.parquet")
PIPELINE_MODEL_PATH = os.path.join(MODEL_DIR, "feature_pipeline_model")


def create_spark_session() -> SparkSession:
    spark = (
        SparkSession.builder
        .appName("so_survey_step_3_gold_features_advanced")
        .master("local[*]")
        .config("spark.sql.shuffle.partitions", "32")
        .config("spark.driver.memory", "4g")
        .config("spark.sql.legacy.timeParserPolicy", "LEGACY")
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("WARN")
    return spark


def choose_numeric_columns(df: DataFrame) -> List[str]:
    """Chá»n cá»™t sá»‘: Chá»‰ dÃ¹ng YearsCodePro (Kinh nghiá»‡m chuyÃªn nghiá»‡p)"""
    # Æ¯u tiÃªn cá»™t Ä‘Ã£ Ä‘Æ°á»£c impute (Ä‘iá»n thiáº¿u)
    candidates = ["YearsCodeProNum_imp", "YearsCodeProNum"]
    for c in candidates:
        if c in df.columns:
            print(f"ğŸ”¢ Numeric feature Ä‘Æ°á»£c chá»n: {c}")
            return [c]
    print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y YearsCodeProNum!")
    return []


def choose_categorical_columns(df: DataFrame) -> List[str]:
    """Chá»n cá»™t phÃ¢n loáº¡i (Categorical)"""
    candidate_cats = [
        "CountryGrouped", # Quá»‘c gia (Ä‘Ã£ gom nhÃ³m)
        "EdLevel",        # TrÃ¬nh Ä‘á»™ há»c váº¥n
        "OrgSize",        # Quy mÃ´ cÃ´ng ty
        "RemoteWork",     # LÃ m tá»« xa hay táº¡i vÄƒn phÃ²ng
    ]
    categorical_cols = [c for c in candidate_cats if c in df.columns]
    print("ğŸ”¤ Categorical features Ä‘Æ°á»£c chá»n:", categorical_cols)
    return categorical_cols


def choose_text_columns(df: DataFrame) -> List[str]:
    """Chá»n cá»™t vÄƒn báº£n (Text) chá»©a danh sÃ¡ch ká»¹ nÄƒng"""
    candidates = ["LanguageHaveWorkedWith", "DatabaseHaveWorkedWith"]
    text_cols = [c for c in candidates if c in df.columns]
    print("ğŸ“ Text features Ä‘Æ°á»£c chá»n:", text_cols)
    return text_cols


def build_pipeline(categorical_cols: List[str], numeric_cols: List[str], text_cols: List[str]) -> Pipeline:
    """
    XÃ¢y dá»±ng chuá»—i xá»­ lÃ½ (Pipeline) gá»“m nhiá»u bÆ°á»›c ná»‘i tiáº¿p nhau.
    """
    stages = []
    assembler_inputs = []

    # 1. Xá»­ lÃ½ Categorical: StringIndexer -> OneHotEncoder
    # StringIndexer: Biáº¿n Ä‘á»•i chuá»—i thÃ nh chá»‰ sá»‘ (VD: USA -> 0, India -> 1)
    # OneHotEncoder: Biáº¿n Ä‘á»•i chá»‰ sá»‘ thÃ nh vector nhá»‹ phÃ¢n (VD: 0 -> [1,0,0], 1 -> [0,1,0])
    for c in categorical_cols:
        idx_col = f"{c}_idx"
        vec_col = f"{c}_vec"
        
        # handleInvalid="keep": Náº¿u gáº·p giÃ¡ trá»‹ láº¡ chÆ°a tá»«ng tháº¥y lÃºc train, váº«n giá»¯ láº¡i (thÃ nh vector 0 háº¿t)
        stages.append(StringIndexer(inputCol=c, outputCol=idx_col, handleInvalid="keep"))
        stages.append(OneHotEncoder(inputCols=[idx_col], outputCols=[vec_col], handleInvalid="keep"))
        assembler_inputs.append(vec_col)

    # 2. Xá»­ lÃ½ Text: RegexTokenizer -> CountVectorizer
    # RegexTokenizer: TÃ¡ch chuá»—i "Python;Java" thÃ nh máº£ng ["Python", "Java"] dá»±a trÃªn dáº¥u cháº¥m pháº©y
    # CountVectorizer: Äáº¿m táº§n suáº¥t xuáº¥t hiá»‡n cá»§a tá»« khÃ³a (Bag of Words)
    for c in text_cols:
        tok_col = f"{c}_tokens"
        vec_col = f"{c}_counts"
        
        stages.append(RegexTokenizer(inputCol=c, outputCol=tok_col, pattern=r";"))
        # vocabSize=30: Chá»‰ láº¥y Top 30 tá»« phá»• biáº¿n nháº¥t Ä‘á»ƒ trÃ¡nh vector quÃ¡ lá»›n
        stages.append(CountVectorizer(inputCol=tok_col, outputCol=vec_col, vocabSize=30, minDF=0.01))
        assembler_inputs.append(vec_col)

    # 3. ThÃªm cá»™t sá»‘ vÃ o danh sÃ¡ch Ä‘áº§u vÃ o
    assembler_inputs.extend(numeric_cols)

    # 4. VectorAssembler: Gom táº¥t cáº£ cÃ¡c cá»™t (sá»‘, one-hot, text vector) thÃ nh 1 vector duy nháº¥t gá»i lÃ  "features_raw"
    print("ğŸ”— CÃ¡c cá»™t Ä‘áº§u vÃ o cho VectorAssembler:", assembler_inputs)
    stages.append(VectorAssembler(
        inputCols=assembler_inputs,
        outputCol="features_raw",
        handleInvalid="keep"
    ))

    # 5. StandardScaler: Chuáº©n hÃ³a dá»¯ liá»‡u vá» cÃ¹ng má»™t thang Ä‘o (Mean=0, Std=1)
    # GiÃºp thuáº­t toÃ¡n Linear Regression há»™i tá»¥ nhanh hÆ¡n vÃ  chÃ­nh xÃ¡c hÆ¡n
    stages.append(StandardScaler(
        inputCol="features_raw",
        outputCol="features",
        withMean=False, # Giá»¯ nguyÃªn tÃ­nh thÆ°a (sparsity) cá»§a vector Ä‘á»ƒ tiáº¿t kiá»‡m RAM
        withStd=True
    ))

    return Pipeline(stages=stages)


def main():
    spark = create_spark_session()

    if not os.path.exists(SILVER_PATH):
        raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y SILVER_PATH: {SILVER_PATH}")

    print(f"ğŸ“¥ Äang Ä‘á»c dá»¯ liá»‡u Silver tá»«: {SILVER_PATH}")
    df = spark.read.parquet(SILVER_PATH)

    # Lá»c bá» dÃ²ng thiáº¿u lÆ°Æ¡ng hoáº·c lÆ°Æ¡ng <= 0
    if "ConvertedCompYearly" not in df.columns:
        raise ValueError("âŒ Thiáº¿u cá»™t ConvertedCompYearly")
    
    df = df.filter(F.col("ConvertedCompYearly").isNotNull() & (F.col("ConvertedCompYearly") > 0))

    # Chá»n features
    numeric_cols = choose_numeric_columns(df)
    categorical_cols = choose_categorical_columns(df)
    text_cols = choose_text_columns(df)

    # Chá»‰ giá»¯ láº¡i cá»™t cáº§n thiáº¿t
    keep_cols = ["ConvertedCompYearly"] + numeric_cols + categorical_cols + text_cols
    df = df.select(*keep_cols)
    df = df.cache() # Cache vÃ o RAM Ä‘á»ƒ cháº¡y nhanh hÆ¡n
    print(f"ğŸ“Š Sá»‘ lÆ°á»£ng máº«u training: {df.count()}")

    # XÃ¢y dá»±ng vÃ  Huáº¥n luyá»‡n Pipeline (Fit)
    # LÃºc nÃ y Spark sáº½ há»c tá»« Ä‘iá»ƒn (Vocabulary) vÃ  cÃ¡c chá»‰ sá»‘ (String Index) tá»« dá»¯ liá»‡u
    pipeline = build_pipeline(categorical_cols, numeric_cols, text_cols)
    model = pipeline.fit(df)

    # Biáº¿n Ä‘á»•i dá»¯ liá»‡u (Transform)
    out = model.transform(df).select("features", "ConvertedCompYearly")

    # Ghi dá»¯ liá»‡u Ä‘Ã£ biáº¿n Ä‘á»•i ra file (Ä‘á»ƒ train model á»Ÿ bÆ°á»›c sau)
    os.makedirs(os.path.dirname(FEATURES_PATH), exist_ok=True)
    out.repartition(32).write.mode("overwrite").parquet(FEATURES_PATH)
    print(f"âœ… ÄÃ£ ghi GOLD features ra: {FEATURES_PATH}")

    # Ghi Pipeline Model ra file (Ä‘á»ƒ Web App dÃ¹ng láº¡i)
    os.makedirs(MODEL_DIR, exist_ok=True)
    model.write().overwrite().save(PIPELINE_MODEL_PATH)
    print(f"âœ… ÄÃ£ lÆ°u Feature Pipeline Model ra: {PIPELINE_MODEL_PATH}")

    spark.stop()


if __name__ == "__main__":
    main()
