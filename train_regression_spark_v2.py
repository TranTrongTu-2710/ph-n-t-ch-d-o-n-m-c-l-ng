# train_regression_spark_v2.py
"""
Train m√¥ h√¨nh d·ª± b√°o l∆∞∆°ng (Spark MLlib) - Phi√™n b·∫£n n√¢ng c·∫•p.

M·ª§C TI√äU:
- ƒê·ªçc d·ªØ li·ªáu Gold (ƒë√£ c√≥ vector features).
- Chia t·∫≠p Train/Test (80/20).
- Hu·∫•n luy·ªán 2 m√¥ h√¨nh: Linear Regression v√† GBT Regressor.
- ƒê√°nh gi√° sai s·ªë (RMSE, MAE).
- L∆∞u m√¥ h√¨nh t·ªët nh·∫•t.
"""

import os
from pyspark.sql import SparkSession, functions as F
from pyspark.ml.regression import LinearRegression, GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator

# ===== C·∫§U H√åNH =====
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data2")
FEATURES_PATH = os.path.join(DATA_DIR, "salary_features_spark.parquet")
MODEL_DIR = os.path.join(BASE_DIR, "models")

def create_spark():
    spark = (
        SparkSession.builder
        .appName("Train_Regression_Spark_v2_Advanced")
        .master("local[*]")
        .config("spark.sql.shuffle.partitions", "32")
        .config("spark.driver.memory", "4g")
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("WARN")
    return spark

def evaluate_real_scale(pred_df):
    """
    H√†m ƒë√°nh gi√° sai s·ªë tr√™n thang ƒëo th·ª±c t·∫ø (USD).
    V√¨ m√¥ h√¨nh d·ª± ƒëo√°n log(l∆∞∆°ng), n√™n c·∫ßn expm1() ƒë·ªÉ ƒë·ªïi v·ªÅ l∆∞∆°ng th·∫≠t.
    """
    df2 = (
        pred_df
        .withColumn("salary_true", F.expm1(F.col("label")))      # L∆∞∆°ng th·∫≠t = exp(log_l∆∞∆°ng) - 1
        .withColumn("salary_pred", F.expm1(F.col("prediction"))) # L∆∞∆°ng d·ª± ƒëo√°n = exp(log_d·ª±_ƒëo√°n) - 1
        .filter(F.col("salary_pred").isNotNull())
    )
    # MAE: Sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh (Trung b√¨nh l·ªách bao nhi√™u USD)
    mae = df2.select(F.avg(F.abs(F.col("salary_true") - F.col("salary_pred"))).alias("mae")).collect()[0]["mae"]
    # RMSE: CƒÉn b·∫≠c hai c·ªßa sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh (Ph·∫°t n·∫∑ng c√°c sai s·ªë l·ªõn)
    rmse = df2.select(F.sqrt(F.avg((F.col("salary_true") - F.col("salary_pred"))**2)).alias("rmse")).collect()[0]["rmse"]
    return float(mae), float(rmse)

def main():
    spark = create_spark()

    if not os.path.exists(FEATURES_PATH):
        raise FileNotFoundError(f"Missing: {FEATURES_PATH}")

    print(f"üì• ƒêang ƒë·ªçc features t·ª´: {FEATURES_PATH}")
    df = spark.read.parquet(FEATURES_PATH)

    # L·ªçc d·ªØ li·ªáu h·ª£p l·ªá
    df = df.filter(F.col("ConvertedCompYearly").isNotNull() & (F.col("ConvertedCompYearly") > 0))

    # ‚úÖ Outlier filter (1% - 99%): L·ªçc b·ªè 1% l∆∞∆°ng th·∫•p nh·∫•t v√† cao nh·∫•t ƒë·ªÉ m√¥ h√¨nh ·ªïn ƒë·ªãnh h∆°n
    q1, q99 = df.approxQuantile("ConvertedCompYearly", [0.01, 0.99], 0.01)
    df = df.filter((F.col("ConvertedCompYearly") >= F.lit(q1)) & (F.col("ConvertedCompYearly") <= F.lit(q99)))

    # Chuy·ªÉn label sang log scale (log1p)
    # L√Ω do: L∆∞∆°ng th∆∞·ªùng ph√¢n ph·ªëi l·ªách ph·∫£i (Right-skewed), log gi√∫p n√≥ ph√¢n ph·ªëi chu·∫©n h∆°n -> T·ªët cho Linear Regression
    df = (df
          .withColumn("label", F.log1p(F.col("ConvertedCompYearly")))
          .select("features", "label")
          .cache())
    
    print("Rows after outlier filter:", df.count())
    print(f"Quantile range (Salary): {q1:.2f} to {q99:.2f}")

    # Chia t·∫≠p Train (80%) v√† Test (20%)
    train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)
    train_df, test_df = train_df.cache(), test_df.cache()
    print(f"Train: {train_df.count()} | Test: {test_df.count()}")

    evaluator_rmse = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="rmse")
    evaluator_r2 = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="r2")

    os.makedirs(MODEL_DIR, exist_ok=True)

    # =========================
    # MODEL 1: Linear Regression
    # =========================
    print("\nüöÄ Training LinearRegression...")
    # ElasticNet: K·∫øt h·ª£p L1 (Lasso) v√† L2 (Ridge) regularization ƒë·ªÉ tr√°nh overfitting
    lr = LinearRegression(featuresCol="features", labelCol="label", maxIter=100, regParam=0.01, elasticNetParam=0.5)
    lr_model = lr.fit(train_df)
    
    pred_lr = lr_model.transform(test_df)
    rmse_lr = evaluator_rmse.evaluate(pred_lr)
    r2_lr = evaluator_r2.evaluate(pred_lr)
    mae_real_lr, rmse_real_lr = evaluate_real_scale(pred_lr)

    print(f"[LR] RMSE(log): {rmse_lr:.4f} | R2(log): {r2_lr:.4f}")
    print(f"[LR] MAE(real): {mae_real_lr:,.2f} | RMSE(real): {rmse_real_lr:,.2f}")
    
    # L∆∞u model LR
    lr_model.write().overwrite().save(os.path.join(MODEL_DIR, "lr_model"))

    # =========================
    # MODEL 2: GBT Regressor (Gradient Boosted Trees)
    # =========================
    print("\nüöÄ Training GBTRegressor (Tuned)...")
    # GBT th∆∞·ªùng t·ªët h∆°n LR v√¨ b·∫Øt ƒë∆∞·ª£c c√°c m·ªëi quan h·ªá phi tuy·∫øn t√≠nh (Non-linear)
    # maxDepth=8: C√¢y s√¢u h∆°n ƒë·ªÉ h·ªçc c√°c pattern ph·ª©c t·∫°p
    gbt = GBTRegressor(
        featuresCol="features", 
        labelCol="label", 
        maxIter=80, 
        maxDepth=8, 
        stepSize=0.1, 
        subsamplingRate=0.8,
        seed=42
    )
    gbt_model = gbt.fit(train_df)
    
    pred_gbt = gbt_model.transform(test_df)
    rmse_gbt = evaluator_rmse.evaluate(pred_gbt)
    r2_gbt = evaluator_r2.evaluate(pred_gbt)
    mae_real_gbt, rmse_real_gbt = evaluate_real_scale(pred_gbt)

    print(f"[GBT] RMSE(log): {rmse_gbt:.4f} | R2(log): {r2_gbt:.4f}")
    print(f"[GBT] MAE(real): {mae_real_gbt:,.2f} | RMSE(real): {rmse_real_gbt:,.2f}")

    # L∆∞u model GBT
    gbt_model.write().overwrite().save(os.path.join(MODEL_DIR, "gbt_model"))

    # So s√°nh v√† k·∫øt lu·∫≠n
    best = ("LR", rmse_lr) if rmse_lr <= rmse_gbt else ("GBT", rmse_gbt)
    print(f"\nüèÜ Best model by RMSE(log): {best[0]} (RMSE={best[1]:.4f})")

    spark.stop()

if __name__ == "__main__":
    main()
