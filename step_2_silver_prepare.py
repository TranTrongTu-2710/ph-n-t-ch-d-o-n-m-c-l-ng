"""
B∆∞·ªõc 2 - Silver: L√†m s·∫°ch v√† Chu·∫©n h√≥a d·ªØ li·ªáu.

M·ª§C TI√äU:
- L·ªçc d·ªØ li·ªáu: Ch·ªâ l·∫•y Developer chuy√™n nghi·ªáp (b·ªè sinh vi√™n, ng∆∞·ªùi l√†m hobby).
- X·ª≠ l√Ω Outlier: B·ªè nh·ªØng m·ª©c l∆∞∆°ng qu√° ·∫£o (qu√° th·∫•p ho·∫∑c qu√° cao).
- Feature Engineering:
    + Chuy·ªÉn ƒë·ªïi c·ªôt chu·ªói "10-20 years" th√†nh s·ªë.
    + Gom nh√≥m c√°c qu·ªëc gia √≠t m·∫´u th√†nh "Other".
    + ƒêi·ªÅn d·ªØ li·ªáu thi·∫øu (Imputation).
"""

import os
import re
from typing import Dict

from pyspark.sql import SparkSession, DataFrame, functions as F, types as T
from pyspark.ml.feature import Imputer


# ================== C·∫§U H√åNH ==================
BRONZE_DIR = os.path.join("data2", "bronze")
SILVER_DIR = os.path.join("data2", "silver")
YEARS = [2021, 2022, 2023, 2024]


def create_spark_session() -> SparkSession:
    spark = (
        SparkSession.builder
        .appName("so_survey_step_2_silver_prepare")
        .master("local[*]")
        .config("spark.sql.legacy.timeParserPolicy", "LEGACY")
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("WARN")
    return spark


def load_bronze_year(spark: SparkSession, year: int) -> DataFrame:
    """ƒê·ªçc d·ªØ li·ªáu Bronze ƒë√£ l∆∞u ·ªü b∆∞·ªõc 1"""
    path = os.path.join(BRONZE_DIR, f"stackoverflow_{year}")
    print(f"üì• ƒêang ƒë·ªçc bronze nƒÉm {year} t·ª´: {path}")
    df = spark.read.parquet(path)
    # Ph√≤ng h·ªù n·∫øu b∆∞·ªõc 1 qu√™n th√™m SurveyYear
    if "SurveyYear" not in df.columns:
        df = df.withColumn("SurveyYear", F.lit(year))
    return df


def union_all_years(dfs_by_year: Dict[int, DataFrame]) -> DataFrame:
    """G·ªôp 4 nƒÉm l·∫°i th√†nh 1 DataFrame duy nh·∫•t"""
    it = iter(dfs_by_year.values())
    df_all = next(it)
    for df in it:
        df_all = df_all.unionByName(df, allowMissingColumns=True)
    return df_all


def build_dev_population(df: DataFrame) -> DataFrame:
    """
    L·ªåC D·ªÆ LI·ªÜU (FILTERING):
    Ch·ªâ gi·ªØ l·∫°i nh·ªØng d√≤ng th·ªèa m√£n ti√™u ch√≠ l√† "Developer chuy√™n nghi·ªáp".
    """
    cond = F.lit(True)

    # 1. MainBranch ph·∫£i ch·ª©a ch·ªØ "developer"
    if "MainBranch" in df.columns:
        cond = cond & F.lower(F.col("MainBranch")).contains("developer")

    # 2. Employment ph·∫£i l√† "employed" (ƒëang ƒëi l√†m)
    if "Employment" in df.columns:
        cond = cond & F.lower(F.col("Employment")).contains("employed")

    # 3. DevType ph·∫£i ch·ª©a c√°c t·ª´ kh√≥a li√™n quan ƒë·∫øn l·∫≠p tr√¨nh
    if "DevType" in df.columns:
        keywords = ["developer", "engineer", "devops", "sre", "data scientist", "machine learning"]
        pattern = "(?i)(" + "|".join(keywords) + ")" # Regex: (?i) nghƒ©a l√† kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng
        cond = cond & F.col("DevType").rlike(pattern)

    # 4. Ph·∫£i c√≥ th√¥ng tin v·ªÅ l∆∞∆°ng (Target kh√¥ng ƒë∆∞·ª£c null)
    if "ConvertedCompYearly" in df.columns:
        cond = cond & F.col("ConvertedCompYearly").isNotNull()

    df_filtered = df.filter(cond)
    print("üë®‚Äçüíª S·ªë d√≤ng sau khi l·ªçc dev ng√†nh ph·∫ßn m·ªÅm:", df_filtered.count())
    return df_filtered


def clean_salary_outliers(df: DataFrame) -> DataFrame:
    """
    LO·∫†I B·ªé OUTLIER (Nhi·ªÖu):
    L∆∞∆°ng qu√° th·∫•p (v√≠ d·ª• $1/nƒÉm) ho·∫∑c qu√° cao (v√≠ d·ª• $100M/nƒÉm) s·∫Ω l√†m h·ªèng m√¥ h√¨nh.
    D√πng ph∆∞∆°ng ph√°p Quantile ƒë·ªÉ c·∫Øt b·ªè 1% ƒë·∫ßu v√† 1% ƒëu√¥i.
    """
    if "ConvertedCompYearly" not in df.columns:
        return df

    # T√≠nh ng∆∞·ª°ng 1% v√† 99%
    low, high = df.approxQuantile("ConvertedCompYearly", [0.01, 0.99], 0.01)
    print(f"‚û°Ô∏è Gi·ªØ l·∫°i l∆∞∆°ng trong kho·∫£ng [{low:.2f}, {high:.2f}]")
    
    df_filtered = df.filter(
        (F.col("ConvertedCompYearly") >= F.lit(low)) &
        (F.col("ConvertedCompYearly") <= F.lit(high))
    )
    print("üíæ S·ªë d√≤ng sau khi lo·∫°i outlier l∆∞∆°ng:", df_filtered.count())
    return df_filtered


# ---------- UDF (User Defined Function) ƒë·ªÉ x·ª≠ l√Ω chu·ªói ----------

def _parse_years_code(value: str):
    """
    Chuy·ªÉn ƒë·ªïi chu·ªói kinh nghi·ªám th√†nh s·ªë.
    VD: 'Less than 1 year' -> 0.5
        'More than 50 years' -> 50.0
        '10' -> 10.0
    """
    if value is None: return None
    v = str(value).strip()
    if v == "" or v.lower() == "na": return None
    lower = v.lower()
    
    if "less than 1 year" in lower: return 0.5
    if "more than" in lower:
        nums = re.findall(r"\d+", v) # T√¨m s·ªë trong chu·ªói
        return float(nums[0]) if nums else None
    
    # Tr∆∞·ªùng h·ª£p b√¨nh th∆∞·ªùng (l√† s·ªë)
    try:
        return float(v)
    except:
        # Tr∆∞·ªùng h·ª£p l·ªói kh√°c, c·ªë g·∫Øng t√¨m s·ªë ƒë·∫ßu ti√™n
        nums = re.findall(r"\d+", v)
        return float(nums[0]) if nums else None

# ƒêƒÉng k√Ω h√†m Python th√†nh h√†m Spark UDF
parse_years_code_udf = F.udf(_parse_years_code, T.DoubleType())


def _parse_age_to_numeric(age_str: str):
    """
    Chuy·ªÉn ƒë·ªïi kho·∫£ng tu·ªïi th√†nh s·ªë trung b√¨nh.
    VD: '25-34 years old' -> 29.5
    """
    if age_str is None: return None
    s = str(age_str)
    if "Under 18" in s: return 17.0
    if "65 years or older" in s: return 65.0
    
    nums = re.findall(r"\d+", s)
    if not nums: return None
    
    if len(nums) == 1: return float(nums[0])
    
    # L·∫•y trung b√¨nh c·ªông c·ªßa kho·∫£ng (VD: 25 v√† 34)
    a, b = float(nums[0]), float(nums[1])
    return (a + b) / 2.0

parse_age_udf = F.udf(_parse_age_to_numeric, T.DoubleType())


def group_small_countries(df: DataFrame, min_count: int = 100) -> DataFrame:
    """
    Gom c√°c qu·ªëc gia c√≥ √≠t h∆°n 100 m·∫´u th√†nh nh√≥m 'Other'.
    Gi√∫p gi·∫£m s·ªë l∆∞·ª£ng bi·∫øn (dimension reduction) khi One-Hot Encoding sau n√†y.
    """
    if "Country" not in df.columns: return df
    
    country_counts = df.groupBy("Country").count()
    
    # T√¨m danh s√°ch c√°c n∆∞·ªõc nh·ªè
    small_countries = [
        row["Country"] for row in country_counts
        .filter(F.col("count") < F.lit(min_count))
        .collect()
    ]
    
    if not small_countries:
        return df.withColumn("CountryGrouped", F.col("Country"))

    # Thay th·∫ø t√™n n∆∞·ªõc b·∫±ng 'Other' n·∫øu n·∫±m trong danh s√°ch nh·ªè
    df_grouped = df.withColumn(
        "CountryGrouped",
        F.when(F.col("Country").isin(small_countries), F.lit("Other"))
         .otherwise(F.col("Country"))
    )
    return df_grouped


def impute_numeric_features(df: DataFrame) -> DataFrame:
    """
    ƒêi·ªÅn d·ªØ li·ªáu thi·∫øu (Missing Value Imputation) cho c√°c c·ªôt s·ªë.
    Chi·∫øn l∆∞·ª£c: D√πng gi√° tr·ªã trung v·ªã (median) c·ªßa c·ªôt ƒë√≥.
    """
    numeric_cols = []
    for c in ["YearsCodeProNum", "YearsCodeNum", "AgeNum"]:
        if c in df.columns: numeric_cols.append(c)

    if not numeric_cols: return df

    # T·∫°o t√™n c·ªôt m·ªõi c√≥ h·∫≠u t·ªë _imp
    output_cols = [c + "_imp" for c in numeric_cols]
    print("üõ† ƒêang impute median cho:", numeric_cols)
    
    imputer = Imputer(inputCols=numeric_cols, outputCols=output_cols).setStrategy("median")
    model = imputer.fit(df)
    return model.transform(df)


def clean_text_columns(df: DataFrame) -> DataFrame:
    """
    ƒêi·ªÅn gi√° tr·ªã r·ªóng cho c√°c c·ªôt k·ªπ nƒÉng (Language, Database) ƒë·ªÉ tr√°nh l·ªói NullPointerException.
    """
    text_cols = ["LanguageHaveWorkedWith", "DatabaseHaveWorkedWith"]
    fill_dict = {}
    for c in text_cols:
        if c in df.columns:
            fill_dict[c] = "None" # N·∫øu null th√¨ ƒëi·ªÅn chu·ªói "None"
    
    if fill_dict:
        print(f"üßπ FillNA cho c√°c c·ªôt text: {list(fill_dict.keys())}")
        df = df.fillna(fill_dict)
    
    return df


# ================== MAIN ==================

def main():
    os.makedirs(SILVER_DIR, exist_ok=True)
    spark = create_spark_session()

    # 1. ƒê·ªçc v√† g·ªôp d·ªØ li·ªáu 4 nƒÉm
    dfs_by_year = {year: load_bronze_year(spark, year) for year in YEARS}
    df_all_raw = union_all_years(dfs_by_year)

    # 2. Ch·ªçn c√°c c·ªôt c·∫ßn thi·∫øt (Feature Selection)
    core_cols = [
        "SurveyYear", "ConvertedCompYearly", "CompTotal", "Country",
        "MainBranch", "DevType", "Employment",
        "YearsCodePro", "YearsCode", "Age", "EdLevel", "OrgSize", "RemoteWork",
        "LanguageHaveWorkedWith", "DatabaseHaveWorkedWith" # C·ªôt k·ªπ nƒÉng m·ªõi th√™m
    ]
    # Ch·ªâ l·∫•y nh·ªØng c·ªôt th·ª±c s·ª± t·ªìn t·∫°i trong d·ªØ li·ªáu
    available_core_cols = [c for c in core_cols if c in df_all_raw.columns]
    df_core = df_all_raw.select(*available_core_cols)

    # 3. L·ªçc Dev v√† lo·∫°i b·ªè Outlier l∆∞∆°ng
    df_dev = build_dev_population(df_core)
    df_dev = clean_salary_outliers(df_dev)

    # 4. √Åp d·ª•ng UDF ƒë·ªÉ chuy·ªÉn ƒë·ªïi c·ªôt s·ªë
    if "YearsCodePro" in df_dev.columns:
        df_dev = df_dev.withColumn("YearsCodeProNum", parse_years_code_udf(F.col("YearsCodePro")))
    if "YearsCode" in df_dev.columns:
        df_dev = df_dev.withColumn("YearsCodeNum", parse_years_code_udf(F.col("YearsCode")))
    if "Age" in df_dev.columns:
        df_dev = df_dev.withColumn("AgeNum", parse_age_udf(F.col("Age")))

    # 5. Gom nh√≥m Country v√† ƒêi·ªÅn d·ªØ li·ªáu thi·∫øu
    df_dev = group_small_countries(df_dev, min_count=100)
    df_dev = impute_numeric_features(df_dev)

    # 6. X·ª≠ l√Ω c·ªôt Text (K·ªπ nƒÉng)
    df_dev = clean_text_columns(df_dev)

    # 7. L∆∞u k·∫øt qu·∫£ ra Silver
    out_path = os.path.join(SILVER_DIR, "stackoverflow_silver_dev_clean.parquet")
    print(f"üíæ Ghi dataset Silver v√†o: {out_path}")
    
    df_dev.write.mode("overwrite").parquet(out_path)

    spark.stop()
    print("\n‚úÖ Ho√†n th√†nh B∆∞·ªõc 2 - Silver.")


if __name__ == "__main__":
    main()
