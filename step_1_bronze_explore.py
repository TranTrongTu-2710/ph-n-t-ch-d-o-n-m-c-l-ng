"""
BÆ°á»›c 1 - Bronze + khÃ¡m phÃ¡ dá»¯ liá»‡u StackOverflow Survey 2021-2024.

Má»¤C TIÃŠU:
- Äá»c dá»¯ liá»‡u thÃ´ (CSV) náº·ng ná».
- Chuáº©n hÃ³a sÆ¡ bá»™ (thÃªm nÄƒm, Ã©p kiá»ƒu sá»‘).
- LÆ°u sang Ä‘á»‹nh dáº¡ng Parquet (Bronze Layer) Ä‘á»ƒ Spark Ä‘á»c nhanh hÆ¡n á»Ÿ cÃ¡c bÆ°á»›c sau.
"""

import os
from pyspark.sql import SparkSession, functions as F, types as T


# ================== Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN ==================
# Sá»­ dá»¥ng os.path.join Ä‘á»ƒ Ä‘Æ°á»ng dáº«n hoáº¡t Ä‘á»™ng Ä‘Ãºng trÃªn cáº£ Windows/Linux/Mac
RAW_DATA_DIR = os.path.join("data2", "raw")
BRONZE_DIR = os.path.join("data2", "bronze")

# Map nÄƒm -> tÃªn file CSV tÆ°Æ¡ng á»©ng
FILE_MAP = {
    2021: "survey_results_public_2021.csv",
    2022: "survey_results_public_2022.csv",
    2023: "survey_results_public_2023.csv",
    2024: "survey_results_public_2024.csv",
}


# ================== HÃ€M Há»– TRá»¢ ==================

def create_spark_session() -> SparkSession:
    """
    Khá»Ÿi táº¡o SparkSession - Ä‘iá»ƒm báº¯t Ä‘áº§u cá»§a má»i á»©ng dá»¥ng Spark.
    """
    spark = (
        SparkSession.builder
        .appName("so_survey_step_1_bronze_explore")
        # local[*]: Cháº¡y trÃªn mÃ¡y cÃ¡ nhÃ¢n, dÃ¹ng táº¥t cáº£ cÃ¡c nhÃ¢n CPU Ä‘ang cÃ³
        .master("local[*]")
        # LEGACY: Äá»ƒ Spark xá»­ lÃ½ Ä‘á»‹nh dáº¡ng ngÃ y thÃ¡ng kiá»ƒu cÅ© náº¿u cÃ³ lá»—i parse
        .config("spark.sql.legacy.timeParserPolicy", "LEGACY")
        .getOrCreate()
    )
    # Chá»‰ hiá»‡n cáº£nh bÃ¡o (WARN) hoáº·c lá»—i (ERROR), áº©n cÃ¡c log thÃ´ng tin (INFO) rÃ¡c
    spark.sparkContext.setLogLevel("WARN")
    return spark


def load_one_year(spark: SparkSession, year: int, filename: str):
    """
    Äá»c 1 file CSV, xá»­ lÃ½ sÆ¡ bá»™ vÃ  tráº£ vá» DataFrame.
    """
    path = os.path.join(RAW_DATA_DIR, filename)
    print(f"ğŸ“¥ Äang Ä‘á»c file nÄƒm {year}: {path}")

    # Äá»c CSV vá»›i cÃ¡c tÃ¹y chá»n quan trá»ng
    df = (
        spark.read
        .option("header", True)       # DÃ²ng Ä‘áº§u tiÃªn lÃ  tÃªn cá»™t
        .option("inferSchema", True)  # Spark tá»± Ä‘oÃ¡n kiá»ƒu dá»¯ liá»‡u (int, string...)
        .option("multiLine", True)    # Cho phÃ©p 1 dÃ²ng dá»¯ liá»‡u xuá»‘ng dÃ²ng (quan trá»ng vá»›i cá»™t text dÃ i)
        .option("escape", "\"")       # Xá»­ lÃ½ kÃ½ tá»± Ä‘áº·c biá»‡t trong chuá»—i
        .csv(path)
    )

    # ThÃªm cá»™t SurveyYear Ä‘á»ƒ sau nÃ y gá»™p nhiá»u nÄƒm láº¡i váº«n biáº¿t dÃ²ng nÃ o cá»§a nÄƒm nÃ o
    df = df.withColumn("SurveyYear", F.lit(year))

    # Ã‰p kiá»ƒu cá»™t lÆ°Æ¡ng vá» sá»‘ thá»±c (Double) Ä‘á»ƒ trÃ¡nh lá»—i tÃ­nh toÃ¡n sau nÃ y
    for col_name in ["ConvertedCompYearly", "CompTotal"]:
        if col_name in df.columns:
            df = df.withColumn(col_name, F.col(col_name).cast(T.DoubleType()))

    return df


def show_basic_stats(df, year: int):
    """
    In ra thá»‘ng kÃª nhanh Ä‘á»ƒ kiá»ƒm tra dá»¯ liá»‡u cÃ³ á»•n khÃ´ng.
    """
    print(f"\n===== Thá»‘ng kÃª nhanh cho nÄƒm {year} =====")
    total_rows = df.count()
    print(f"ğŸ”¢ Tá»•ng sá»‘ dÃ²ng: {total_rows}")

    # Kiá»ƒm tra cá»™t lÆ°Æ¡ng (Target Variable)
    if "ConvertedCompYearly" in df.columns:
        salary_not_null = df.filter(F.col("ConvertedCompYearly").isNotNull())
        salary_null = total_rows - salary_not_null.count()

        print(f"ğŸ’° Sá»‘ dÃ²ng cÃ³ lÆ°Æ¡ng: {salary_not_null.count()}")
        print(f"ğŸš« Sá»‘ dÃ²ng thiáº¿u lÆ°Æ¡ng: {salary_null}")

        # TÃ­nh trung bÃ¬nh vÃ  trung vá»‹ (median) lÆ°Æ¡ng
        if salary_not_null.count() > 0:
            agg = (
                salary_not_null
                    .agg(
                        F.mean("ConvertedCompYearly").alias("mean_salary"),
                        # percentile_approx(0.5) chÃ­nh lÃ  Median
                        F.expr("percentile_approx(ConvertedCompYearly, 0.5)").alias("median_salary")
                    )
                    .collect()[0]
            )
            print(f"ğŸ“Š LÆ°Æ¡ng trung bÃ¬nh: {agg['mean_salary']:.2f}")
            print(f"ğŸ“Š LÆ°Æ¡ng median:    {agg['median_salary']:.2f}")
    else:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y cá»™t ConvertedCompYearly.")


def save_bronze(df, year: int):
    """
    LÆ°u DataFrame ra file Parquet.
    Parquet nÃ©n tá»‘t hÆ¡n CSV vÃ  giá»¯ Ä‘Æ°á»£c kiá»ƒu dá»¯ liá»‡u (schema).
    """
    out_path = os.path.join(BRONZE_DIR, f"stackoverflow_{year}")
    print(f"ğŸ’¾ Ghi dá»¯ liá»‡u nÄƒm {year} vÃ o: {out_path}")

    (
        df.write
        .mode("overwrite")  # Ghi Ä‘Ã¨ náº¿u file Ä‘Ã£ tá»“n táº¡i
        .parquet(out_path)
    )


def union_all_years(dfs):
    """
    Gá»™p dá»¯ liá»‡u cÃ¡c nÄƒm láº¡i thÃ nh 1 DataFrame lá»›n.
    """
    it = iter(dfs.values())
    df_all = next(it)
    for df in it:
        # allowMissingColumns=True: Náº¿u nÄƒm 2021 thiáº¿u cá»™t mÃ  2022 cÃ³, Spark váº«n cho gá»™p (Ä‘iá»n null)
        df_all = df_all.unionByName(df, allowMissingColumns=True)
    return df_all


# ================== MAIN ==================

def main():
    # Táº¡o thÆ° má»¥c chá»©a dá»¯ liá»‡u náº¿u chÆ°a cÃ³
    os.makedirs(BRONZE_DIR, exist_ok=True)

    spark = create_spark_session()
    dfs_by_year = {}

    # 1. VÃ²ng láº·p xá»­ lÃ½ tá»«ng nÄƒm
    for year, filename in FILE_MAP.items():
        df_year = load_one_year(spark, year, filename)
        dfs_by_year[year] = df_year

        show_basic_stats(df_year, year)
        save_bronze(df_year, year) # LÆ°u ngay sau khi Ä‘á»c

    # 2. Thá»­ gá»™p láº¡i Ä‘á»ƒ xem thá»‘ng kÃª tá»•ng quan
    print("\n===== Thá»‘ng kÃª tá»•ng há»£p lÆ°Æ¡ng theo nÄƒm (tá»« Bronze) =====")
    df_all = union_all_years(dfs_by_year)

    if "ConvertedCompYearly" in df_all.columns:
        stats_by_year = (
            df_all
            .groupBy("SurveyYear")
            .agg(
                F.count("*").alias("n_rows"),
                F.count(F.col("ConvertedCompYearly")).alias("n_salary_not_null"),
                F.mean("ConvertedCompYearly").alias("mean_salary"),
                F.expr("percentile_approx(ConvertedCompYearly, 0.5)").alias("median_salary"),
            )
            .orderBy("SurveyYear")
        )
        stats_by_year.show(truncate=False)

    spark.stop()
    print("\nâœ… HoÃ n thÃ nh BÆ°á»›c 1 (Bronze).")


if __name__ == "__main__":
    main()
